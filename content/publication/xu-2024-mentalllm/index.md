---
title: 'Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data'
subtitle: ''
summary: ''
authors:
- Xuhai Xu
- Bingsheng Yao
- Yuanzhe Dong
- Saadia Gabriel
- Hong Yu
- James Hendler
- Marzyeh Ghassemi
- Anind K Dey
- admin
tags: []
categories: []
date: '2024-01-01'
lastmod: 2024-05-01T19:01:43-04:00
featured: false
draft: false

image:
  caption: ''
  focal_point: ''
  preview_only: false

projects: []
publishDate: '2024-05-01T23:01:43.257235Z'
publication_types:
- '2'
abstract: "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research."
publication: "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
url_pdf: 'https://dl.acm.org/doi/pdf/10.1145/3643540'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
---